{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using VineyardRuntime for Efficient Intermediate Data Management in Data Processing Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "Today's BigData/AI applications often need to be realized using an end-to-end pipeline, as exemplified by the data operation flow of a risk control job shown in the following figure: First, order-related data needs to be exported from the database; subsequently, the graph computation engine will process these raw data to construct a user-product relationship graph and, through the graph algorithm, initially screen out the potential cheating gangs that are hidden therein; next, the machine learning algorithm will attribute cheating to these potential gangs and screen out more accurate results; finally, these results will be screened manually and finally make business processing.\n",
    "\n",
    "![Workflow](./static/workflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In such a scenario, we often encounter the following problems:\n",
    "1. Differences between development and production environments make the development and debugging of data workflows complex and inefficient:\n",
    "\n",
    "    Data scientists develop data operations on their own computers using Python code, but then need to convert the code into YAML files in production environments that they are not familiar with to utilize Kubernetes-based workflow engines such as Argo, Tekton, and so on, which greatly reduces the efficiency of development and deployment, and brings risks resulting from the discrepancy between development and production environments.\n",
    "\n",
    "2. It is necessary to introduce new distributed storage to realize intermediate temporary data exchange, which brings additional development, expense, operation and maintenance costs:\n",
    "\n",
    "    The data exchange between subtasks of end-to-end tasks usually relies on distributed file systems or object storage systems (e.g., HDFS, S3, OSS), which makes the whole workflow require a lot of data format conversion and adaptation work, resulting in redundant I/O operations, and the use of a distributed storage system leads to additional costs due to the short-term nature of intermediate data.\n",
    "\n",
    "3. The efficiency of data processing in a large-scale Kubernetes cluster environment:\n",
    "\n",
    "    When using the existing distributed file system to process data in a large-scale Kubernetes cluster, the scheduling system lacks sufficient understanding of the read and write locality of the data, does not effectively consider the location of the data, and does not fully utilize the locality of the data, which results in the inability to avoid a large number of repetitive data pulling operations when processing data exchanges between nodes. This operation increases the I/O consumption and reduces the overall operational efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![workflow with vineyard](./static/workflow_with_vineyard.png)\n",
    "\n",
    "To address the above problems with existing data flow operations in Big Data/AI, we combine Vineyard's data sharing mechanisms with Fluid's data orchestration capabilities.\n",
    "1. Fluid's Python SDK enables easy orchestration of data flow, providing data scientists skilled in Python with an easy way to build and commit workflows centered on dataset operations. Specifically, data flows are managed through a single set of code in both development and production environments on the cloud.\n",
    "2. Vineyard makes data sharing between tasks in end-to-end workflows more efficient by enabling zero-copy data sharing through memory mapping, which avoids the additional IO overheads that are key to data sharing efficiency gains.\n",
    "3. By utilizing Fluid's data affinity scheduling capability, the Pod scheduling policy takes into account information about which nodes the data is written to, thus reducing the network overhead introduced by data migration and improving end-to-end performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Example\n",
    "\n",
    "In the next example, we will use VineyardRuntime and DataFlow in Fluid to show how to achieve efficient intermediate data management in the data processing pipeline.DataFlow is a built-in data flow orchestration capability provided by Fluid, which allows you to chain multiple data operations in the data processing pipeline to achieve a simple logical orchestration. For more advanced workflow orchestration capabilities, VineyardRuntime also supports integration with workflow orchestration engines such as Argo Workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install oss2 numpy pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OSS_ACCESS_KEY_ID\"] = \"<YOUR_ACCESS_KEY>\"\n",
    "os.environ[\"OSS_ACCESS_KEY_SECRET\"] = \"<YOUR_ACCESS_SECRET>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Fake data\n",
    "num_rows = 600 * 1000\n",
    "df = pd.DataFrame({\n",
    "    'Id': np.random.randint(1, 100000, num_rows),\n",
    "    'MSSubClass': np.random.randint(20, 201, size=num_rows),\n",
    "    'LotFrontage': np.random.randint(50, 151, size=num_rows),\n",
    "    'LotArea': np.random.randint(5000, 20001, size=num_rows),\n",
    "    'OverallQual': np.random.randint(1, 11, size=num_rows),\n",
    "    'OverallCond': np.random.randint(1, 11, size=num_rows),\n",
    "    'YearBuilt': np.random.randint(1900, 2022, size=num_rows),\n",
    "    'YearRemodAdd': np.random.randint(1900, 2022, size=num_rows),\n",
    "    'MasVnrArea': np.random.randint(0, 1001, size=num_rows),\n",
    "    'BsmtFinSF1': np.random.randint(0, 2001, size=num_rows),\n",
    "    'BsmtFinSF2': np.random.randint(0, 1001, size=num_rows),\n",
    "    'BsmtUnfSF': np.random.randint(0, 2001, size=num_rows),\n",
    "    'TotalBsmtSF': np.random.randint(0, 3001, size=num_rows),\n",
    "    '1stFlrSF': np.random.randint(500, 4001, size=num_rows),\n",
    "    '2ndFlrSF': np.random.randint(0, 2001, size=num_rows),\n",
    "    'LowQualFinSF': np.random.randint(0, 201, size=num_rows),\n",
    "    'GrLivArea': np.random.randint(600, 5001, size=num_rows),\n",
    "    'BsmtFullBath': np.random.randint(0, 4, size=num_rows),\n",
    "    'BsmtHalfBath': np.random.randint(0, 3, size=num_rows),\n",
    "    'FullBath': np.random.randint(0, 5, size=num_rows),\n",
    "    'HalfBath': np.random.randint(0, 3, size=num_rows),\n",
    "    'BedroomAbvGr': np.random.randint(0, 11, size=num_rows),\n",
    "    'KitchenAbvGr': np.random.randint(0, 4, size=num_rows),\n",
    "    'TotRmsAbvGrd': np.random.randint(0, 16, size=num_rows),\n",
    "    'Fireplaces': np.random.randint(0, 4, size=num_rows),\n",
    "    'GarageYrBlt': np.random.randint(1900, 2022, size=num_rows),\n",
    "    'GarageCars': np.random.randint(0, 5, num_rows),\n",
    "    'GarageArea': np.random.randint(0, 1001, num_rows),\n",
    "    'WoodDeckSF': np.random.randint(0, 501, num_rows),\n",
    "    'OpenPorchSF': np.random.randint(0, 301, num_rows),\n",
    "    'EnclosedPorch': np.random.randint(0, 201, num_rows),\n",
    "    '3SsnPorch': np.random.randint(0, 101, num_rows),\n",
    "    'ScreenPorch': np.random.randint(0, 201, num_rows),\n",
    "    'PoolArea': np.random.randint(0, 301, num_rows),\n",
    "    'MiscVal': np.random.randint(0, 5001, num_rows),\n",
    "    'TotalRooms': np.random.randint(2, 11, num_rows),\n",
    "    \"GarageAge\": np.random.randint(1, 31, num_rows),\n",
    "    \"RemodAge\": np.random.randint(1, 31, num_rows),\n",
    "    \"HouseAge\": np.random.randint(1, 31, num_rows),\n",
    "    \"TotalBath\": np.random.randint(1, 5, num_rows),\n",
    "    \"TotalPorchSF\": np.random.randint(1, 1001, num_rows),\n",
    "    \"TotalSF\": np.random.randint(1000, 6001, num_rows),\n",
    "    \"TotalArea\": np.random.randint(1000, 6001, num_rows),\n",
    "    'MoSold': np.random.randint(1, 13, num_rows),\n",
    "    'YrSold': np.random.randint(2006, 2022, num_rows),\n",
    "    'SalePrice': np.random.randint(50000, 800001, num_rows),\n",
    "})\n",
    "\n",
    "print(\"DataFrame initialized.\")\n",
    "\n",
    "import oss2\n",
    "import io\n",
    "from oss2.credentials import EnvironmentVariableCredentialsProvider\n",
    "# Please set your OSS accessKeyID and accessKeySecret to the environment variables OSS_ACCESS_KEY_ID and OSS_ACCESS_KEY_SECRET respectively.\n",
    "auth = oss2.ProviderAuth(EnvironmentVariableCredentialsProvider())\n",
    "# Please replace OSS_ENDPOINT and BUCKET_NAME with your OSS Endpoint and Bucket\n",
    "bucket = oss2.Bucket(auth, '<OSS_ENDPOINT>', '<OSS_BUCKET_NAME>')\n",
    "\n",
    "bytes_buffer = io.BytesIO()\n",
    "df.to_pickle(bytes_buffer)\n",
    "bucket.put_object(\"df.pkl\", bytes_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create Fluid Dataset and VineyardRuntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting fluidsdk logger level to DEBUG for detailed messages\n",
    "import logging\n",
    "import sys\n",
    "logger = logging.getLogger(\"fluidsdk\")\n",
    "stream_handler = logging.StreamHandler(sys.stdout)\n",
    "stream_handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))\n",
    "logger.addHandler(stream_handler)\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-08 12:08:26,145 - fluidsdk - DEBUG - Dataset \"default/vineyard\" created\n"
     ]
    }
   ],
   "source": [
    "import fluid\n",
    "\n",
    "from fluid import constants\n",
    "from fluid import models\n",
    "\n",
    "# Connect to the Fluid using the default kubeconfig file and create a Fluid client instance\n",
    "client_config = fluid.ClientConfig()\n",
    "fluid_client = fluid.FluidClient(client_config)\n",
    "\n",
    "# Create a Dataset named vineyard under default namespace\n",
    "fluid_client.create_dataset(\n",
    "    dataset_name=\"vineyard\",\n",
    ")\n",
    "\n",
    "# Get the vineyard Dataset\n",
    "dataset = fluid_client.get_dataset(dataset_name=\"vineyard\")\n",
    "\n",
    "# Initialize the configuration of the vineyard runtime and bind the vineyard Dataset to that runtime.\n",
    "# The number of replicas is 2 and the memory is 30Gi respectively\n",
    "dataset.bind_runtime(\n",
    "    runtime_type=constants.VINEYARD_RUNTIME_KIND,\n",
    "    replicas=2,\n",
    "    cache_capacity_GiB=30,\n",
    "    cache_medium=\"MEM\",\n",
    "    wait=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code snippet:\n",
    "- Creating Fluid Client: This code is responsible for establishing a connection to the Fluid using the default kubeconfig file and creating an instance of the Fluid client.\n",
    "- Creating and Configuring the vineyard Dataset and Runtime: Next, the code creates a Dataset named Vineyard, then gets that Dataset and initializes the vineyard Runtime, setting the number of replicas and memory size and binding the dataset to the runtime environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Define the Fluid DataFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kubernetes.client import models as k8s_models\n",
    "# Define the task template and mount the OSS Volume\n",
    "def create_processor(process_func, packages_to_install, pip_index_url):\n",
    "    extra_volumes = k8s_models.V1Volume(\n",
    "                name=\"data\",\n",
    "                persistent_volume_claim=k8s_models.V1PersistentVolumeClaimVolumeSource(\n",
    "                    claim_name=\"pvc-oss\"\n",
    "                )\n",
    "            )\n",
    "    extra_volume_mount = k8s_models.V1VolumeMount(\n",
    "                name=\"data\",\n",
    "                mount_path=\"/data\"\n",
    "            )\n",
    "    \n",
    "    from fluid.utils import processor as processor_utils\n",
    "    debug_mode = True # Setting debug_mode to True for verbose\n",
    "    processor = processor_utils.make_processor_from_func(process_func, packages_to_install=packages_to_install, pip_index_url=pip_index_url, volumes=[extra_volumes], volume_mounts=[extra_volume_mount], debug_mode=debug_mode)\n",
    "\n",
    "    return processor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code snippet:\n",
    "- **Creating a task template:** The code encapsulates a task template function called `create_processor`, which takes a Python function object and automatically parses the contents of the code in the Python function object, and finally passes in the code as a startup command for some container. The `create_processor` function also sets the Python version (defaults to version 3.10, refer to `processor_utils.make_processor_from_func` for method signatures) and PyPI dependencies required to run the function. The container will also mount the OSS storage data source in the `/data` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: Before mounting an OSS storage data source you need to create a PersistentVolumeClaim (PVC) resource named `pvc-oss` in the cluster in advance and bind it to an OSS type PersistentVolume (PV). The PV needs to specify the path to the Bucket that is uploaded in the Dataset Preparation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data preprocessing scripts\n",
    "def preprocess():\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    import pandas as pd\n",
    "    import vineyard\n",
    "    \n",
    "    df = pd.read_pickle('/data/df.pkl')\n",
    "    \n",
    "    # Preprocess Data\n",
    "    df = df.drop(df[(df['GrLivArea']>4800)].index)\n",
    "    X = df.drop('SalePrice', axis=1)  # Features\n",
    "    y = df['SalePrice']  # Target variable\n",
    "    \n",
    "    del df\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    \n",
    "    del X, y\n",
    "    \n",
    "    vineyard.put(X_train, name=\"x_train\", persist=True)\n",
    "    vineyard.put(X_test, name=\"x_test\", persist=True)\n",
    "    vineyard.put(y_train, name=\"y_train\", persist=True)\n",
    "    vineyard.put(y_test, name=\"y_test\", persist=True)\n",
    "\n",
    "\n",
    "def train():\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "\n",
    "    import joblib\n",
    "    import pandas as pd\n",
    "    import vineyard\n",
    "\n",
    "    x_train_data = vineyard.get(name=\"x_train\", fetch=True)\n",
    "    y_train_data = vineyard.get(name=\"y_train\", fetch=True)\n",
    "\n",
    "    model = LinearRegression()\n",
    "    model.fit(x_train_data, y_train_data)\n",
    "\n",
    "    joblib.dump(model, '/data/model.pkl')\n",
    "\n",
    "\n",
    "def test():\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "\n",
    "    import vineyard\n",
    "    import joblib\n",
    "    import pandas as pd\n",
    "\n",
    "    x_test_data = vineyard.get(name=\"x_test\", fetch=True)\n",
    "    y_test_data = vineyard.get(name=\"y_test\", fetch=True)\n",
    "\n",
    "    model = joblib.load(\"/data/model.pkl\")\n",
    "    y_pred = model.predict(x_test_data)\n",
    "\n",
    "    err = mean_squared_error(y_test_data, y_pred)\n",
    "\n",
    "    with open('/data/output.txt', 'a') as f:\n",
    "        f.write(str(err))\n",
    "\n",
    "\n",
    "packages_to_install = [\"numpy\", \"pandas\", \"pyarrow\", \"requests\", \"vineyard\", \"scikit-learn==1.4.0\", \"joblib==1.3.2\"]\n",
    "pip_index_url = \"https://pypi.tuna.tsinghua.edu.cn/simple\"\n",
    "\n",
    "preprocess_processor = create_processor(preprocess, packages_to_install, pip_index_url)\n",
    "train_processor = create_processor(train, packages_to_install, pip_index_url)\n",
    "test_processor = create_processor(test, packages_to_install, pip_index_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code snippets define three steps in the data processing pipeline: data preprocessing, model training, and model testing, respectively. The Python functions corresponding to these three steps are passed into the `create_processor` function to be encapsulated into three processors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task workflow for creating a linear regression model: data preprocessing -> model training -> model testing\n",
    "# The following mount path \"/var/run\" is the default path for vineyard configuration files\n",
    "flow = dataset.process(processor=preprocess_processor, dataset_mountpath=\"/var/run\") \\\n",
    "              .process(processor=train_processor, dataset_mountpath=\"/var/run\") \\\n",
    "              .process(processor=test_processor, dataset_mountpath=\"/var/run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-08 12:13:09,983 - fluidsdk - INFO - DataProcess linear-regression-with-vineyard-step1 completed\n",
      "2024-03-08 12:15:26,417 - fluidsdk - INFO - DataProcess linear-regression-with-vineyard-step2 completed\n",
      "2024-03-08 12:17:39,682 - fluidsdk - INFO - DataProcess linear-regression-with-vineyard-step3 completed\n"
     ]
    }
   ],
   "source": [
    "# Submit the data processing workflow for the linear regression model and wait for its completion\n",
    "run = flow.run(run_id=\"linear-regression-with-vineyard\")\n",
    "run.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 清理所有资源\n",
    "dataset.clean_up(wait=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
